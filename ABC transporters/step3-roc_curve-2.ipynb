{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing machine learning model to classify if a drug is more efficacious in the ABC-16 strain relative to the parental strain using the presence of MACCS fingerprints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt1\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the excel file\n",
    "dataset = pd.read_excel (r'..data/MACCSbinary.xlsx', sheet_name='MACCSbinary')\n",
    "# droping the test set with NA labels\n",
    "df_train =dataset.dropna()\n",
    "X = df_train.iloc[:, 3:152]\n",
    "Y = df_train.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the data\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier_knn = KNeighborsClassifier(n_neighbors = 20, metric = 'minkowski', p = 2)\n",
    "classifier_knn.fit(X_train, y_train)\n",
    "# predicting the test set\n",
    "y_pred_knn = classifier_knn.predict(X_test)\n",
    "y_test_knn = y_test.values\n",
    "#y_pred_probs\n",
    "y_knn_probs = classifier_knn.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the fpr,tpr and area under the curve values for the knn implementation\n",
    "fpr_knn, tpr_knn, thresholds_knn = roc_curve(y_test_knn, y_knn_probs[:,1], pos_label = \"Sensitive\")\n",
    "roc_auc_knn = auc(fpr_knn, tpr_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier_log = LogisticRegression(solver = \"liblinear\",random_state = 0, penalty = \"l1\")\n",
    "classifier_log.fit(X_train, y_train)\n",
    "y_pred_log = classifier_log.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_log = y_test.values\n",
    "#y_pred_probs\n",
    "y_probs_log = classifier_log.predict_proba(X_test)\n",
    "fpr_log, tpr_log, thresholds_log = roc_curve(y_test_log, y_probs_log [:,1], pos_label = \"Sensitive\")\n",
    "roc_auc_log = auc(fpr_log, tpr_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBM methods\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "classifier_GBM = GradientBoostingClassifier(n_estimators= 50, learning_rate=1.0, max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "classifier_GBM.score(X_test, y_test)\n",
    "y_pred_GBM = classifier_GBM.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_GBM = y_test.values\n",
    "#y_pred_probs\n",
    "y_GBM_probs = classifier_GBM.predict_proba(X_test)\n",
    "fpr_GBM, tpr_GBM, thresholds_GBM = roc_curve(y_test_GBM, y_GBM_probs[:,1], pos_label = \"Sensitive\")\n",
    "roc_auc_GBM = auc(fpr_GBM, tpr_GBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM model\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier_svm = SVC(kernel = \"sigmoid\", random_state = 0, probability=True)\n",
    "classifier_svm.fit(X_train, y_train)\n",
    "classifier_svm.score(X_test, y_test)\n",
    "y_pred_svm = classifier_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_SVM = y_test.values\n",
    "#y_pred_probs\n",
    "y_SVM_probs = classifier_svm.predict_proba(X_test)\n",
    "fpr_svm, tpr_svm, thresholds_svm = roc_curve(y_test_SVM, y_SVM_probs[:,1], pos_label = \"Sensitive\")\n",
    "roc_auc_svm = auc(fpr_svm, tpr_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier_NB = GaussianNB()\n",
    "classifier_NB.fit(X_train, y_train)\n",
    "y_pred_NB = classifier_NB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_nb = y_test.values\n",
    "#y_pred_probs\n",
    "y_nb_probs = classifier_NB.predict_proba(X_test)\n",
    "fpr_nb, tpr_nb, thresholds_nb = roc_curve(y_test_nb, y_nb_probs[:,1], pos_label = \"Sensitive\")\n",
    "roc_auc_nb = auc(fpr_nb, tpr_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision trees\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier_dt = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier_dt.fit(X_train, y_train)\n",
    "y_pred_dt = classifier_dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_dt = y_test.values\n",
    "#y_pred_probs\n",
    "y_dt_probs = classifier_dt.predict_proba(X_test)\n",
    "fpr_dt, tpr_dt, thresholds_dt = roc_curve(y_test_dt, y_dt_probs[:,1], pos_label = \"Sensitive\")\n",
    "roc_auc_dt = auc(fpr_dt, tpr_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6), dpi=80)\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr_knn, tpr_knn, 'b', label = 'AUC of KNN = %0.2f' % roc_auc_knn, color='yellow')\n",
    "plt.plot(fpr_log, tpr_log, 'b', label = 'AUC of Logistic = %0.2f' % roc_auc_log, color='red')\n",
    "plt.plot(fpr_GBM, tpr_GBM, 'b', label = 'AUC of GBM = %0.2f' % roc_auc_GBM, color='pink')\n",
    "plt.plot(fpr_svm, tpr_svm, 'b', label = 'AUC of SVM = %0.2f' % roc_auc_svm, color = \"blue\")\n",
    "plt.plot(fpr_nb, tpr_nb, 'b', label = 'AUC of NB = %0.2f' % roc_auc_nb, color = \"orange\")\n",
    "plt.plot(fpr_dt, tpr_dt, 'b', label = 'AUC of Decision tree = %0.2f' % roc_auc_dt, color =\"maroon\")\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the AUC of the 6 models, KNN with 20 nearest neighbors and GBM with 50 estimators and depth of 1 gives the best classification for the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt1.figure(figsize=(8, 6), dpi=80)\n",
    "plot_precision_recall_curve(classifier_knn, X_test, y_test,ax = plt1.gca())\n",
    "\n",
    "plot_precision_recall_curve(classifier_log, X_test, y_test, ax = plt1.gca())\n",
    "\n",
    "plot_precision_recall_curve(classifier_GBM, X_test, y_test, ax = plt1.gca())\n",
    "plot_precision_recall_curve(classifier_svm, X_test, y_test, ax = plt1.gca())\n",
    "plot_precision_recall_curve(classifier_NB, X_test, y_test, ax = plt1.gca())\n",
    "plot_precision_recall_curve(classifier_dt, X_test, y_test, ax = plt1.gca())\n",
    "plt1.legend(loc = 'lower right')\n",
    "plt1.title('Precision-Recall curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here again, the Knn and GBM models gives the best preformance based on the AP of the Precision-Recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the accuracy of the model\n",
    "y_pred = [y_pred_knn, y_pred_log, y_pred_GBM, y_pred_svm, y_pred_NB, y_pred_dt]\n",
    "method = [\"KNN\",\"Logistic\",\"GBM\", \"SVM\", \"Naive Bayes\",\"Decision Trees\"]\n",
    "acc = []\n",
    "zip_object = zip(y_pred,method)\n",
    "#y_test = y_test.values\n",
    "\n",
    "for i,j in zip_object:\n",
    "    print(\"Accuracy of \" + str(j) + \" is \" + str(accuracy_score(y_test, i)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN classifier gives the highest accuracy of 0.73 followed by GBM with an accuracy of 0.70"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
